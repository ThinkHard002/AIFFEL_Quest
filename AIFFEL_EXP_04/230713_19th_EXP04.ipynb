{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNVPd5jgNj0kkiGHGfOjBPL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 7. 영화리뷰 텍스트 감성분석하기\n","\n"],"metadata":{"id":"gjWTvwYMtKa1"}},{"cell_type":"markdown","source":["\n","---\n","* 7-01. 들어가며\n","* 7-02. 텍스트 감정분석의 유용성\n","* 7-03. 텍스트 데이터의 특징\n","* 7-04. 텍스트 데이터의 특징 (1) 텍스트를 숫자로 표현하는 방법\n","* 7-05. 텍스트 데이터의 특징 (2) Embedding 레이어의 등장\n","* 7-06. 시퀀스 데이터를 다루는 RNN\n","* 7-07. 꼭 RNN이어야 할까?\n","* 7-08. IMDB 영화리뷰 감성분석 (1) IMDB 데이터셋 분석\n","* 7-09. IMDB 영화리뷰 감성분석 (2) 딥러닝 모델 설계와 훈련\n","* 7-10. IMDB 영화리뷰 감성분석 (3) Word2Vec의 적용\n","---\n","\n"],"metadata":{"id":"sY2bbj-8YWTN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDoy7n88ryUI"},"outputs":[],"source":["# Google drive 연결\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"markdown","source":["## 7-02. 텍스트 감정분석의 유용성\n","\n","* 동아비즈니스리뷰 감성분석 활용 사례 기고\n","\n","https://dbr.donga.com/article/view/1202/article_no/8891/ac/magazine\n","\n"],"metadata":{"id":"4gXdpSQpX4mR"}},{"cell_type":"markdown","source":["## 7-03. 텍스트 데이터의 특징"],"metadata":{"id":"2DUhzzUYja5j"}},{"cell_type":"markdown","source":["## 7-04. 텍스트 데이터의 특징 (1) 텍스트를 숫자로 표현하는 방법"],"metadata":{"id":"2H3gfx2rFIHp"}},{"cell_type":"code","source":["# 처리해야 할 문장을 파이썬 리스트에 옮겨 담았습니다.\n","sentences=['i feel hungry', 'i eat lunch', 'now i feel happy']\n","\n","# 파이썬 split() 메소드를 이용해 단어 단위로 문장을 쪼개 봅니다.\n","word_list = 'i feel hungry'.split()\n","print(word_list)"],"metadata":{"id":"-wRnnMi-Zzui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_word={}  # 빈 딕셔너리를 만들어서\n","\n","# 단어들을 하나씩 채워 봅니다. 채우는 순서는 일단 임의로 하였습니다. 그러나 사실 순서는 중요하지 않습니다.\n","# <BOS>, <PAD>, <UNK>는 관례적으로 딕셔너리 맨 앞에 넣어줍니다.\n","index_to_word[0]='<PAD>'  # 패딩용 단어\n","index_to_word[1]='<BOS>'  # 문장의 시작지점\n","index_to_word[2]='<UNK>'  # 사전에 없는(Unknown) 단어\n","index_to_word[3]='i'\n","index_to_word[4]='feel'\n","index_to_word[5]='hungry'\n","index_to_word[6]='eat'\n","index_to_word[7]='lunch'\n","index_to_word[8]='now'\n","index_to_word[9]='happy'\n","\n","print(index_to_word)"],"metadata":{"id":"NKrGeFV2Z2TW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_to_index={word:index for index, word in index_to_word.items()}\n","print(word_to_index)"],"metadata":{"id":"oDCLrve-Z2li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(word_to_index['feel'])  # 단어 'feel'은 숫자 인덱스 4로 바뀝니다."],"metadata":{"id":"CLS1gwlMZ2zo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수를 만들어 봅시다.\n","# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다.\n","def get_encoded_sentence(sentence, word_to_index):\n","    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n","\n","print(get_encoded_sentence('i eat lunch', word_to_index))"],"metadata":{"id":"KuzasHiJZ3PO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode해 주는 함수입니다.\n","def get_encoded_sentences(sentences, word_to_index):\n","    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n","\n","# sentences=['i feel hungry', 'i eat lunch', 'now i feel happy'] 가 아래와 같이 변환됩니다.\n","encoded_sentences = get_encoded_sentences(sentences, word_to_index)\n","print(encoded_sentences)"],"metadata":{"id":"EGcVUEM4GD5v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다.\n","def get_decoded_sentence(encoded_sentence, index_to_word):\n","    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n","\n","print(get_decoded_sentence([1, 3, 4, 5], index_to_word))"],"metadata":{"id":"A-sHx3WgGEFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다.\n","def get_decoded_sentences(encoded_sentences, index_to_word):\n","    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n","\n","# encoded_sentences=[[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 가 아래와 같이 변환됩니다.\n","print(get_decoded_sentences(encoded_sentences, index_to_word))"],"metadata":{"id":"9Y0GiIX4GEOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YVZj1zu8GEU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-05. 텍스트 데이터의 특징 (2) Embedding 레이어의 등장"],"metadata":{"id":"-Inmw-q_HkM3"}},{"cell_type":"code","source":["# 아래 코드는 그대로 실행하시면 에러가 발생할 것입니다.\n","\n","import numpy as np\n","import tensorflow as tf\n","import os\n","\n","vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n","word_vector_dim = 4    # 위 그림과 같이 4차원의 워드 벡터를 가정합니다.\n","\n","embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n","\n","# 숫자로 변환된 텍스트 데이터 [[1, 3, 4, 5], [1, 3, 6, 7], [1, 8, 3, 4, 9]] 에 Embedding 레이어를 적용합니다.\n","raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype='object')\n","output = embedding(raw_inputs)\n","print(output)"],"metadata":{"id":"fib4BBuGHmkO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["실행해 보니 에러가 발생합니다. 왜 그럴까요?\n","\n","주의해야 할 점이 있습니다. Embedding 레이어의 인풋이 되는 문장 벡터는 그 길이가 일정해야 합니다. raw_inputs의 3개 벡터의 길이는 각각 4, 4, 5입니다.\n","Tensorflow에서는 tf.keras.preprocessing.sequence.pad_sequences라는 편리한 함수를 통해 문장 벡터 뒤에 패딩(<PAD>)을 추가하여 길이를 일정하게 맞춰주는 기능을 제공합니다.\n"],"metadata":{"id":"tsSs0dhNBQYX"}},{"cell_type":"code","source":["raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n","                                                       value=word_to_index['<PAD>'],\n","                                                       padding='post',\n","                                                       maxlen=5)\n","print(raw_inputs)"],"metadata":{"id":"qUH1COCiHmYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(word_to_index)  # 위 예시에서 딕셔너리에 포함된 단어 개수는 10\n","word_vector_dim = 4    # 그림과 같이 4차원의 워드 벡터를 가정합니다.\n","\n","embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=word_vector_dim, mask_zero=True)\n","\n","# tf.keras.preprocessing.sequence.pad_sequences를 통해 word vector를 모두 일정 길이로 맞춰주어야\n","# embedding 레이어의 input이 될 수 있음에 주의해 주세요.\n","raw_inputs = np.array(get_encoded_sentences(sentences, word_to_index), dtype=object)\n","raw_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\n","                                                       value=word_to_index['<PAD>'],\n","                                                       padding='post',\n","                                                       maxlen=5)\n","output = embedding(raw_inputs)\n","print(output)"],"metadata":{"id":"ucRA5K2jxiBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lkfYNilGxjIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-06. 시퀀스 데이터를 다루는 RNN"],"metadata":{"id":"651MwpV5Hm1L"}},{"cell_type":"markdown","source":["김성훈 교수의 모두의 딥러닝 강좌 12강.RNN\n","\n","https://www.youtube.com/watch?v=-SHPG_KMUkQ"],"metadata":{"id":"k5magJEPD_jM"}},{"cell_type":"code","source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n","word_vector_dim = 4  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다.\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"id":"EeC9jx12ysoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uhiPvCoSHnCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b6lIINg2HnGe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-07. 꼭 RNN이어야 할까?"],"metadata":{"id":"QXimKNYjHnV2"}},{"cell_type":"markdown","source":["텍스트를 처리하기 위해 RNN이 아니라 1-D Convolution Neural Network(1-D CNN)를 사용할 수도 있습니다.\n","우리는 이미지 분류기를 구현하면서 2-D CNN을 이미 사용해 본 바 있습니다. 이미지는 시퀀스 데이터가 아닙니다. 이미지 분류기 모델에는 이미지 전체가 한꺼번에 입력으로 사용됩니다.\n","그러므로 1-D CNN은 문장 전체를 한꺼번에 한 방향으로 길이 7짜리 필터로 스캐닝 하면서 7단어 이내에서 발견되는 특징을 추출하여 그것으로 문장을 분류하는 방식으로 사용됩니다. 이 방식도 텍스트를 처리하는 데 RNN 못지않은 효율을 보여줍니다.\n","그리고 CNN 계열은 RNN 계열보다 병렬처리가 효율적이기 때문에 학습 속도도 훨씬 빠르게 진행된다는 장점이 있습니다."],"metadata":{"id":"AMp_fsSYEsP_"}},{"cell_type":"code","source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n","word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다.\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(tf.keras.layers.MaxPooling1D(5))\n","model.add(tf.keras.layers.Conv1D(16, 7, activation='relu'))\n","model.add(tf.keras.layers.GlobalMaxPooling1D())\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"id":"BzmGicvlHngi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["아주 간단히는 GlobalMaxPooling1D() 레이어 하나만 사용하는 방법도 생각해 볼 수 있습니다. 이 방식은 전체 문장 중에서 단 하나의 가장 중요한 단어만 피처로 추출하여 그것으로 문장의 긍정/부정을 평가하는 방식이라고 생각할 수 있는데, 의외로 성능이 잘 나올 수도 있습니다."],"metadata":{"id":"Kj3C5pB_E7vk"}},{"cell_type":"code","source":["vocab_size = 10  # 어휘 사전의 크기입니다(10개의 단어)\n","word_vector_dim = 4   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다.\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n","model.add(tf.keras.layers.GlobalMaxPooling1D())\n","model.add(tf.keras.layers.Dense(8, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n","\n","model.summary()"],"metadata":{"id":"UfYP0LqSHnlC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-08. IMDB 영화리뷰 감성분석 (1) IMDB 데이터셋 분석\n"],"metadata":{"id":"c6B9ExTWHnz4"}},{"cell_type":"code","source":["imdb = tf.keras.datasets.imdb\n","\n","# IMDb 데이터셋 다운로드\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n","print(f\"훈련 샘플 개수: {len(x_train)}, 테스트 개수: {len(x_test)}\")"],"metadata":{"id":"1bmqkTLGHn-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train[0])  # 1번째 리뷰데이터\n","print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨\n","print('1번째 리뷰 문장 길이: ', len(x_train[0]))\n","print('2번째 리뷰 문장 길이: ', len(x_train[1]))"],"metadata":{"id":"G91yNsYgFTgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_to_index = imdb.get_word_index()\n","index_to_word = {index:word for word, index in word_to_index.items()}\n","print(index_to_word[1])     # 'the' 가 출력됩니다.\n","print(word_to_index['the'])  # 1 이 출력됩니다."],"metadata":{"id":"jdl1f77sFYkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 보정 전 x_train[0] 데이터\n","print(get_decoded_sentence(x_train[0], index_to_word))"],"metadata":{"id":"44lxq2jYFZPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.\n","word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n","\n","# 처음 몇 개 인덱스는 사전에 정의되어 있습니다.\n","word_to_index[\"<PAD>\"] = 0\n","word_to_index[\"<BOS>\"] = 1\n","word_to_index[\"<UNK>\"] = 2  # unknown\n","word_to_index[\"<UNUSED>\"] = 3\n","\n","index_to_word = {index:word for word, index in word_to_index.items()}\n","\n","print(index_to_word[1])     # '<BOS>' 가 출력됩니다.\n","print(word_to_index['the'])  # 4 이 출력됩니다.\n","print(index_to_word[4])     # 'the' 가 출력됩니다.\n","\n","# 보정 후 x_train[0] 데이터\n","print(get_decoded_sentence(x_train[0], index_to_word))"],"metadata":{"id":"odPOJAkfFU_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_decoded_sentence(x_train[0], index_to_word))\n","print('라벨: ', y_train[0])  # 1번째 리뷰데이터의 라벨"],"metadata":{"id":"9m6yJONgFVbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_data_text = list(x_train) + list(x_test)\n","# 텍스트데이터 문장길이의 리스트를 생성한 후\n","num_tokens = [len(tokens) for tokens in total_data_text]\n","num_tokens = np.array(num_tokens)\n","# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n","print('문장길이 평균 : ', np.mean(num_tokens))\n","print('문장길이 최대 : ', np.max(num_tokens))\n","print('문장길이 표준편차 : ', np.std(num_tokens))\n","\n","# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,\n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","maxlen = int(max_tokens)\n","print('pad_sequences maxlen : ', maxlen)\n","print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 maxlen 설정값 이내에 포함됩니다. ')"],"metadata":{"id":"NE6K8u0CFVpn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n","                                                        value=word_to_index[\"<PAD>\"],\n","                                                        padding='post', # 혹은 'pre'\n","                                                        maxlen=maxlen)\n","\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n","                                                       value=word_to_index[\"<PAD>\"],\n","                                                       padding='post', # 혹은 'pre'\n","                                                       maxlen=maxlen)\n","\n","print(x_train.shape)"],"metadata":{"id":"h_lcIu4kHoCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QFapGz_jF32M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"igohsTnsF4uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-09. IMDB 영화리뷰 감성분석 (2) 딥러닝 모델 설계와 훈련"],"metadata":{"id":"IKtnxo8dIAoV"}},{"cell_type":"code","source":["vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n","word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n","\n","# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n","model = tf.keras.Sequential()\n","# [[YOUR CODE]]\n","\n","model.summary()"],"metadata":{"id":"edgxtb_GIA_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XgYAEbHrGMTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4X5R8abNGMht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ab_B3utnGMvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PQEBYtVPGNDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iCso4O0wIBDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7-10. IMDB 영화리뷰 감성분석 (3) Word2Vec의 적용"],"metadata":{"id":"7BEVP5lUICC0"}},{"cell_type":"code","source":[],"metadata":{"id":"JM_LSuJ5ICNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1CZwG3sCICSL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8.\n","\n"],"metadata":{"id":"eNerzNddYfVD"}},{"cell_type":"markdown","source":["## TODO"],"metadata":{"id":"1SW5JGbmY1G8"}},{"cell_type":"code","source":[],"metadata":{"id":"G0Ns3GrAYy4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hN2NLcPkY1eM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"APPOiNbZY17B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sauKmuQWZUZa"},"execution_count":null,"outputs":[]}]}